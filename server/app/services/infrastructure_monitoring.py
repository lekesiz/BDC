"""
Infrastructure Monitoring Service
Monitors server metrics, network latency, storage usage, and container health
"""

import time
import json
import logging
import threading
import subprocess
import psutil
import redis
import docker
import requests
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, Any, Optional, List
from dataclasses import dataclass, asdict

from app.utils.logging import logger

@dataclass
class ServerMetric:
    """Server performance metric data structure"""
    timestamp: float
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    network_bytes_sent: int
    network_bytes_recv: int
    load_average: List[float]
    active_connections: int
    
@dataclass
class ContainerMetric:
    """Container performance metric"""
    container_id: str
    name: str
    status: str
    cpu_percent: float
    memory_usage: int
    memory_limit: int
    network_rx: int
    network_tx: int
    block_read: int
    block_write: int

@dataclass
class NetworkLatencyMetric:
    """Network latency measurement"""
    target: str
    timestamp: float
    latency_ms: float
    packet_loss: float
    status: str  # 'healthy', 'degraded', 'unhealthy'

class InfrastructureMonitor:
    """Comprehensive infrastructure monitoring"""
    
    def __init__(self, redis_client: Optional[redis.Redis] = None):
        self.redis_client = redis_client
        
        # Metrics storage
        self.server_metrics = deque(maxlen=1440)  # 24 hours of minutes
        self.container_metrics = deque(maxlen=1000)
        self.network_metrics = deque(maxlen=500)
        self.storage_metrics = deque(maxlen=288)  # 24 hours of 5-minute intervals
        
        # Docker client (if available)
        self.docker_client = None
        try:
            self.docker_client = docker.from_env()
        except Exception as e:\n            logger.warning(f\"Docker not available: {e}\")\n        \n        # Network targets to monitor\n        self.network_targets = [\n            {'name': 'google_dns', 'host': '8.8.8.8'},\n            {'name': 'cloudflare_dns', 'host': '1.1.1.1'},\n            {'name': 'local_gateway', 'host': self._get_default_gateway()}\n        ]\n        \n        # Service dependencies to monitor\n        self.service_dependencies = [\n            {'name': 'database', 'host': 'localhost', 'port': 5432},\n            {'name': 'redis', 'host': 'localhost', 'port': 6379},\n            {'name': 'prometheus', 'host': 'localhost', 'port': 9090},\n            {'name': 'grafana', 'host': 'localhost', 'port': 3000}\n        ]\n        \n        # Monitoring thresholds\n        self.thresholds = {\n            'cpu_warning': 70,\n            'cpu_critical': 85,\n            'memory_warning': 80,\n            'memory_critical': 90,\n            'disk_warning': 80,\n            'disk_critical': 90,\n            'load_warning': 2.0,\n            'load_critical': 4.0,\n            'network_latency_warning': 100,  # ms\n            'network_latency_critical': 500,  # ms\n            'packet_loss_warning': 1.0,  # %\n            'packet_loss_critical': 5.0,  # %\n        }\n        \n        # Monitoring state\n        self._monitoring_active = False\n        self._monitoring_thread = None\n        self._alerts = deque(maxlen=100)\n        \n        logger.info(\"Infrastructure Monitor initialized\")\n    \n    def _get_default_gateway(self) -> str:\n        \"\"\"Get default gateway IP address\"\"\"\n        try:\n            gateways = psutil.net_if_addrs()\n            # This is a simplified approach - in production, use proper routing table lookup\n            return \"192.168.1.1\"  # Common default\n        except Exception:\n            return \"192.168.1.1\"\n    \n    def start_monitoring(self):\n        \"\"\"Start infrastructure monitoring\"\"\"\n        if not self._monitoring_active:\n            self._monitoring_active = True\n            self._monitoring_thread = threading.Thread(\n                target=self._monitoring_loop,\n                daemon=True\n            )\n            self._monitoring_thread.start()\n            logger.info(\"Infrastructure monitoring started\")\n    \n    def stop_monitoring(self):\n        \"\"\"Stop infrastructure monitoring\"\"\"\n        self._monitoring_active = False\n        if self._monitoring_thread:\n            self._monitoring_thread.join(timeout=10)\n            logger.info(\"Infrastructure monitoring stopped\")\n    \n    def _monitoring_loop(self):\n        \"\"\"Main monitoring loop\"\"\"\n        last_server_check = 0\n        last_container_check = 0\n        last_network_check = 0\n        last_storage_check = 0\n        \n        while self._monitoring_active:\n            try:\n                current_time = time.time()\n                \n                # Server metrics every 60 seconds\n                if current_time - last_server_check >= 60:\n                    self._collect_server_metrics()\n                    last_server_check = current_time\n                \n                # Container metrics every 30 seconds\n                if current_time - last_container_check >= 30:\n                    self._collect_container_metrics()\n                    last_container_check = current_time\n                \n                # Network metrics every 120 seconds\n                if current_time - last_network_check >= 120:\n                    self._collect_network_metrics()\n                    last_network_check = current_time\n                \n                # Storage metrics every 300 seconds (5 minutes)\n                if current_time - last_storage_check >= 300:\n                    self._collect_storage_metrics()\n                    last_storage_check = current_time\n                \n                # Check service dependencies\n                self._check_service_dependencies()\n                \n                time.sleep(10)  # Base loop interval\n                \n            except Exception as e:\n                logger.error(f\"Error in infrastructure monitoring loop: {e}\")\n                time.sleep(30)\n    \n    def _collect_server_metrics(self):\n        \"\"\"Collect server performance metrics\"\"\"\n        try:\n            # CPU metrics\n            cpu_percent = psutil.cpu_percent(interval=1)\n            \n            # Memory metrics\n            memory = psutil.virtual_memory()\n            memory_percent = memory.percent\n            \n            # Disk metrics\n            disk = psutil.disk_usage('/')\n            disk_percent = (disk.used / disk.total) * 100\n            \n            # Network metrics\n            network = psutil.net_io_counters()\n            \n            # Load average\n            try:\n                load_avg = list(psutil.getloadavg())\n            except AttributeError:\n                load_avg = [0.0, 0.0, 0.0]\n            \n            # Active connections\n            connections = len(psutil.net_connections())\n            \n            # Create metric object\n            metric = ServerMetric(\n                timestamp=time.time(),\n                cpu_percent=cpu_percent,\n                memory_percent=memory_percent,\n                disk_percent=disk_percent,\n                network_bytes_sent=network.bytes_sent,\n                network_bytes_recv=network.bytes_recv,\n                load_average=load_avg,\n                active_connections=connections\n            )\n            \n            # Store metric\n            self.server_metrics.append(metric)\n            \n            # Check thresholds and create alerts\n            self._check_server_thresholds(metric)\n            \n            # Store in Redis\n            if self.redis_client:\n                self._store_server_metrics_redis(metric)\n                \n        except Exception as e:\n            logger.error(f\"Failed to collect server metrics: {e}\")\n    \n    def _collect_container_metrics(self):\n        \"\"\"Collect Docker container metrics\"\"\"\n        if not self.docker_client:\n            return\n        \n        try:\n            containers = self.docker_client.containers.list()\n            \n            for container in containers:\n                try:\n                    # Get container stats\n                    stats = container.stats(stream=False)\n                    \n                    # Calculate CPU percentage\n                    cpu_delta = stats['cpu_stats']['cpu_usage']['total_usage'] - \\\n                               stats['precpu_stats']['cpu_usage']['total_usage']\n                    system_delta = stats['cpu_stats']['system_cpu_usage'] - \\\n                                  stats['precpu_stats']['system_cpu_usage']\n                    \n                    cpu_percent = 0.0\n                    if system_delta > 0:\n                        cpu_percent = (cpu_delta / system_delta) * \\\n                                     len(stats['cpu_stats']['cpu_usage']['percpu_usage']) * 100\n                    \n                    # Memory metrics\n                    memory_usage = stats['memory_stats']['usage']\n                    memory_limit = stats['memory_stats']['limit']\n                    \n                    # Network metrics\n                    network_rx = 0\n                    network_tx = 0\n                    if 'networks' in stats:\n                        for interface in stats['networks'].values():\n                            network_rx += interface['rx_bytes']\n                            network_tx += interface['tx_bytes']\n                    \n                    # Block I/O metrics\n                    block_read = 0\n                    block_write = 0\n                    if 'blkio_stats' in stats and 'io_service_bytes_recursive' in stats['blkio_stats']:\n                        for io_stat in stats['blkio_stats']['io_service_bytes_recursive']:\n                            if io_stat['op'] == 'Read':\n                                block_read += io_stat['value']\n                            elif io_stat['op'] == 'Write':\n                                block_write += io_stat['value']\n                    \n                    # Create container metric\n                    metric = ContainerMetric(\n                        container_id=container.short_id,\n                        name=container.name,\n                        status=container.status,\n                        cpu_percent=cpu_percent,\n                        memory_usage=memory_usage,\n                        memory_limit=memory_limit,\n                        network_rx=network_rx,\n                        network_tx=network_tx,\n                        block_read=block_read,\n                        block_write=block_write\n                    )\n                    \n                    self.container_metrics.append(metric)\n                    \n                    # Check container health\n                    self._check_container_health(metric)\n                    \n                except Exception as e:\n                    logger.warning(f\"Failed to get stats for container {container.name}: {e}\")\n                    \n        except Exception as e:\n            logger.error(f\"Failed to collect container metrics: {e}\")\n    \n    def _collect_network_metrics(self):\n        \"\"\"Collect network latency and connectivity metrics\"\"\"\n        for target in self.network_targets:\n            try:\n                metric = self._ping_target(target['host'], target['name'])\n                if metric:\n                    self.network_metrics.append(metric)\n                    self._check_network_thresholds(metric)\n                    \n            except Exception as e:\n                logger.error(f\"Failed to ping {target['name']}: {e}\")\n    \n    def _ping_target(self, host: str, name: str) -> Optional[NetworkLatencyMetric]:\n        \"\"\"Ping a target and measure latency\"\"\"\n        try:\n            import platform\n            \n            # Use appropriate ping command based on OS\n            if platform.system().lower() == 'windows':\n                cmd = ['ping', '-n', '4', host]\n            else:\n                cmd = ['ping', '-c', '4', host]\n            \n            start_time = time.time()\n            result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n            \n            if result.returncode == 0:\n                # Parse ping output to get average latency\n                output = result.stdout\n                latency_ms = self._parse_ping_latency(output)\n                packet_loss = self._parse_ping_packet_loss(output)\n                \n                # Determine status\n                status = 'healthy'\n                if latency_ms > self.thresholds['network_latency_critical'] or \\\n                   packet_loss > self.thresholds['packet_loss_critical']:\n                    status = 'unhealthy'\n                elif latency_ms > self.thresholds['network_latency_warning'] or \\\n                     packet_loss > self.thresholds['packet_loss_warning']:\n                    status = 'degraded'\n                \n                return NetworkLatencyMetric(\n                    target=name,\n                    timestamp=time.time(),\n                    latency_ms=latency_ms,\n                    packet_loss=packet_loss,\n                    status=status\n                )\n            else:\n                return NetworkLatencyMetric(\n                    target=name,\n                    timestamp=time.time(),\n                    latency_ms=9999.0,\n                    packet_loss=100.0,\n                    status='unhealthy'\n                )\n                \n        except subprocess.TimeoutExpired:\n            return NetworkLatencyMetric(\n                target=name,\n                timestamp=time.time(),\n                latency_ms=9999.0,\n                packet_loss=100.0,\n                status='unhealthy'\n            )\n        except Exception as e:\n            logger.error(f\"Failed to ping {host}: {e}\")\n            return None\n    \n    def _parse_ping_latency(self, ping_output: str) -> float:\n        \"\"\"Parse average latency from ping output\"\"\"\n        import re\n        \n        # Look for average latency in ping output\n        patterns = [\n            r'avg[^\\d]*([\\d.]+)',  # Linux/Mac format\n            r'Average = ([\\d.]+)ms',  # Windows format\n        ]\n        \n        for pattern in patterns:\n            match = re.search(pattern, ping_output, re.IGNORECASE)\n            if match:\n                return float(match.group(1))\n        \n        return 0.0\n    \n    def _parse_ping_packet_loss(self, ping_output: str) -> float:\n        \"\"\"Parse packet loss percentage from ping output\"\"\"\n        import re\n        \n        # Look for packet loss percentage\n        match = re.search(r'([\\d.]+)%.*loss', ping_output, re.IGNORECASE)\n        if match:\n            return float(match.group(1))\n        \n        return 0.0\n    \n    def _collect_storage_metrics(self):\n        \"\"\"Collect storage usage metrics for all mounted filesystems\"\"\"\n        try:\n            storage_data = []\n            \n            # Get all disk partitions\n            partitions = psutil.disk_partitions()\n            \n            for partition in partitions:\n                try:\n                    usage = psutil.disk_usage(partition.mountpoint)\n                    \n                    storage_info = {\n                        'device': partition.device,\n                        'mountpoint': partition.mountpoint,\n                        'fstype': partition.fstype,\n                        'total_bytes': usage.total,\n                        'used_bytes': usage.used,\n                        'free_bytes': usage.free,\n                        'usage_percent': (usage.used / usage.total) * 100\n                    }\n                    \n                    storage_data.append(storage_info)\n                    \n                    # Check storage thresholds\n                    self._check_storage_thresholds(storage_info)\n                    \n                except PermissionError:\n                    # Skip inaccessible filesystems\n                    continue\n                except Exception as e:\n                    logger.warning(f\"Failed to get usage for {partition.mountpoint}: {e}\")\n            \n            # Store metrics\n            metric = {\n                'timestamp': time.time(),\n                'filesystems': storage_data\n            }\n            \n            self.storage_metrics.append(metric)\n            \n            # Store in Redis\n            if self.redis_client:\n                self._store_storage_metrics_redis(metric)\n                \n        except Exception as e:\n            logger.error(f\"Failed to collect storage metrics: {e}\")\n    \n    def _check_service_dependencies(self):\n        \"\"\"Check if service dependencies are accessible\"\"\"\n        for service in self.service_dependencies:\n            try:\n                # Check if service is listening on specified port\n                import socket\n                \n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.settimeout(5)\n                \n                result = sock.connect_ex((service['host'], service['port']))\n                sock.close()\n                \n                if result != 0:\n                    self._create_alert('service_unavailable', {\n                        'service': service['name'],\n                        'host': service['host'],\n                        'port': service['port'],\n                        'severity': 'high'\n                    })\n                    \n            except Exception as e:\n                logger.error(f\"Failed to check service {service['name']}: {e}\")\n    \n    def _check_server_thresholds(self, metric: ServerMetric):\n        \"\"\"Check server metrics against thresholds\"\"\"\n        alerts = []\n        \n        # CPU threshold\n        if metric.cpu_percent > self.thresholds['cpu_critical']:\n            alerts.append({\n                'type': 'cpu_critical',\n                'value': metric.cpu_percent,\n                'threshold': self.thresholds['cpu_critical'],\n                'severity': 'critical'\n            })\n        elif metric.cpu_percent > self.thresholds['cpu_warning']:\n            alerts.append({\n                'type': 'cpu_warning',\n                'value': metric.cpu_percent,\n                'threshold': self.thresholds['cpu_warning'],\n                'severity': 'warning'\n            })\n        \n        # Memory threshold\n        if metric.memory_percent > self.thresholds['memory_critical']:\n            alerts.append({\n                'type': 'memory_critical',\n                'value': metric.memory_percent,\n                'threshold': self.thresholds['memory_critical'],\n                'severity': 'critical'\n            })\n        elif metric.memory_percent > self.thresholds['memory_warning']:\n            alerts.append({\n                'type': 'memory_warning',\n                'value': metric.memory_percent,\n                'threshold': self.thresholds['memory_warning'],\n                'severity': 'warning'\n            })\n        \n        # Disk threshold\n        if metric.disk_percent > self.thresholds['disk_critical']:\n            alerts.append({\n                'type': 'disk_critical',\n                'value': metric.disk_percent,\n                'threshold': self.thresholds['disk_critical'],\n                'severity': 'critical'\n            })\n        elif metric.disk_percent > self.thresholds['disk_warning']:\n            alerts.append({\n                'type': 'disk_warning',\n                'value': metric.disk_percent,\n                'threshold': self.thresholds['disk_warning'],\n                'severity': 'warning'\n            })\n        \n        # Load average threshold\n        if len(metric.load_average) > 0:\n            load_1min = metric.load_average[0]\n            if load_1min > self.thresholds['load_critical']:\n                alerts.append({\n                    'type': 'load_critical',\n                    'value': load_1min,\n                    'threshold': self.thresholds['load_critical'],\n                    'severity': 'critical'\n                })\n            elif load_1min > self.thresholds['load_warning']:\n                alerts.append({\n                    'type': 'load_warning',\n                    'value': load_1min,\n                    'threshold': self.thresholds['load_warning'],\n                    'severity': 'warning'\n                })\n        \n        # Create alerts\n        for alert in alerts:\n            self._create_alert(alert['type'], alert)\n    \n    def _check_container_health(self, metric: ContainerMetric):\n        \"\"\"Check container health\"\"\"\n        # Check if container is running\n        if metric.status != 'running':\n            self._create_alert('container_not_running', {\n                'container': metric.name,\n                'status': metric.status,\n                'severity': 'high'\n            })\n        \n        # Check container resource usage\n        memory_percent = (metric.memory_usage / metric.memory_limit) * 100\n        \n        if memory_percent > 90:\n            self._create_alert('container_high_memory', {\n                'container': metric.name,\n                'memory_percent': memory_percent,\n                'severity': 'warning'\n            })\n        \n        if metric.cpu_percent > 80:\n            self._create_alert('container_high_cpu', {\n                'container': metric.name,\n                'cpu_percent': metric.cpu_percent,\n                'severity': 'warning'\n            })\n    \n    def _check_network_thresholds(self, metric: NetworkLatencyMetric):\n        \"\"\"Check network metrics against thresholds\"\"\"\n        if metric.status != 'healthy':\n            severity = 'critical' if metric.status == 'unhealthy' else 'warning'\n            \n            self._create_alert('network_latency_high', {\n                'target': metric.target,\n                'latency_ms': metric.latency_ms,\n                'packet_loss': metric.packet_loss,\n                'status': metric.status,\n                'severity': severity\n            })\n    \n    def _check_storage_thresholds(self, storage_info: Dict[str, Any]):\n        \"\"\"Check storage usage thresholds\"\"\"\n        usage_percent = storage_info['usage_percent']\n        \n        if usage_percent > self.thresholds['disk_critical']:\n            self._create_alert('storage_critical', {\n                'device': storage_info['device'],\n                'mountpoint': storage_info['mountpoint'],\n                'usage_percent': usage_percent,\n                'free_gb': storage_info['free_bytes'] / (1024**3),\n                'severity': 'critical'\n            })\n        elif usage_percent > self.thresholds['disk_warning']:\n            self._create_alert('storage_warning', {\n                'device': storage_info['device'],\n                'mountpoint': storage_info['mountpoint'],\n                'usage_percent': usage_percent,\n                'free_gb': storage_info['free_bytes'] / (1024**3),\n                'severity': 'warning'\n            })\n    \n    def _create_alert(self, alert_type: str, data: Dict[str, Any]):\n        \"\"\"Create infrastructure alert\"\"\"\n        alert = {\n            'type': alert_type,\n            'timestamp': time.time(),\n            'data': data,\n            'severity': data.get('severity', 'warning'),\n            'source': 'infrastructure_monitor'\n        }\n        \n        self._alerts.append(alert)\n        \n        # Log alert\n        logger.warning(f\"Infrastructure alert: {alert_type} - {data}\")\n        \n        # Store in Redis\n        if self.redis_client:\n            self.redis_client.lpush(\"infrastructure:alerts\", json.dumps(alert))\n            self.redis_client.ltrim(\"infrastructure:alerts\", 0, 99)\n    \n    def _store_server_metrics_redis(self, metric: ServerMetric):\n        \"\"\"Store server metrics in Redis\"\"\"\n        try:\n            time_key = datetime.now().strftime('%Y%m%d%H')\n            \n            metric_data = asdict(metric)\n            \n            self.redis_client.lpush(\n                f\"infrastructure:server:{time_key}\",\n                json.dumps(metric_data)\n            )\n            \n            # Set expiry (24 hours)\n            self.redis_client.expire(f\"infrastructure:server:{time_key}\", 86400)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store server metrics in Redis: {e}\")\n    \n    def _store_storage_metrics_redis(self, metric: Dict[str, Any]):\n        \"\"\"Store storage metrics in Redis\"\"\"\n        try:\n            time_key = datetime.now().strftime('%Y%m%d%H')\n            \n            self.redis_client.lpush(\n                f\"infrastructure:storage:{time_key}\",\n                json.dumps(metric)\n            )\n            \n            # Set expiry (24 hours)\n            self.redis_client.expire(f\"infrastructure:storage:{time_key}\", 86400)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store storage metrics in Redis: {e}\")\n    \n    def get_infrastructure_summary(self, hours: int = 1) -> Dict[str, Any]:\n        \"\"\"Get infrastructure monitoring summary\"\"\"\n        cutoff_time = time.time() - (hours * 3600)\n        \n        # Filter recent metrics\n        recent_server = [\n            metric for metric in self.server_metrics\n            if metric.timestamp > cutoff_time\n        ]\n        \n        recent_network = [\n            metric for metric in self.network_metrics\n            if metric.timestamp > cutoff_time\n        ]\n        \n        recent_alerts = [\n            alert for alert in self._alerts\n            if alert['timestamp'] > cutoff_time\n        ]\n        \n        # Calculate summaries\n        server_summary = self._calculate_server_summary(recent_server)\n        network_summary = self._calculate_network_summary(recent_network)\n        container_summary = self._get_current_container_summary()\n        storage_summary = self._get_current_storage_summary()\n        \n        return {\n            'timestamp': datetime.utcnow().isoformat(),\n            'period_hours': hours,\n            'server': server_summary,\n            'network': network_summary,\n            'containers': container_summary,\n            'storage': storage_summary,\n            'alerts': recent_alerts[-10:],  # Last 10 alerts\n            'thresholds': self.thresholds\n        }\n    \n    def _calculate_server_summary(self, metrics: List[ServerMetric]) -> Dict[str, Any]:\n        \"\"\"Calculate server performance summary\"\"\"\n        if not metrics:\n            return {'status': 'no_data'}\n        \n        cpu_values = [m.cpu_percent for m in metrics]\n        memory_values = [m.memory_percent for m in metrics]\n        disk_values = [m.disk_percent for m in metrics]\n        \n        return {\n            'status': 'healthy',  # Could be calculated based on thresholds\n            'cpu': {\n                'current': cpu_values[-1] if cpu_values else 0,\n                'average': sum(cpu_values) / len(cpu_values),\n                'peak': max(cpu_values)\n            },\n            'memory': {\n                'current': memory_values[-1] if memory_values else 0,\n                'average': sum(memory_values) / len(memory_values),\n                'peak': max(memory_values)\n            },\n            'disk': {\n                'current': disk_values[-1] if disk_values else 0,\n                'average': sum(disk_values) / len(disk_values),\n                'peak': max(disk_values)\n            },\n            'load_average': metrics[-1].load_average if metrics else [0, 0, 0],\n            'active_connections': metrics[-1].active_connections if metrics else 0\n        }\n    \n    def _calculate_network_summary(self, metrics: List[NetworkLatencyMetric]) -> Dict[str, Any]:\n        \"\"\"Calculate network performance summary\"\"\"\n        if not metrics:\n            return {'status': 'no_data'}\n        \n        # Group by target\n        by_target = defaultdict(list)\n        for metric in metrics:\n            by_target[metric.target].append(metric)\n        \n        target_summaries = {}\n        overall_status = 'healthy'\n        \n        for target, target_metrics in by_target.items():\n            latencies = [m.latency_ms for m in target_metrics if m.latency_ms < 9999]\n            packet_losses = [m.packet_loss for m in target_metrics]\n            \n            if latencies:\n                avg_latency = sum(latencies) / len(latencies)\n                max_latency = max(latencies)\n            else:\n                avg_latency = 9999\n                max_latency = 9999\n            \n            avg_packet_loss = sum(packet_losses) / len(packet_losses)\n            \n            # Determine target status\n            status = 'healthy'\n            if avg_latency > self.thresholds['network_latency_critical'] or \\\n               avg_packet_loss > self.thresholds['packet_loss_critical']:\n                status = 'unhealthy'\n                overall_status = 'unhealthy'\n            elif avg_latency > self.thresholds['network_latency_warning'] or \\\n                 avg_packet_loss > self.thresholds['packet_loss_warning']:\n                status = 'degraded'\n                if overall_status == 'healthy':\n                    overall_status = 'degraded'\n            \n            target_summaries[target] = {\n                'status': status,\n                'average_latency_ms': avg_latency,\n                'max_latency_ms': max_latency,\n                'average_packet_loss': avg_packet_loss,\n                'measurements': len(target_metrics)\n            }\n        \n        return {\n            'status': overall_status,\n            'targets': target_summaries\n        }\n    \n    def _get_current_container_summary(self) -> Dict[str, Any]:\n        \"\"\"Get current container status summary\"\"\"\n        if not self.docker_client:\n            return {'status': 'docker_unavailable'}\n        \n        try:\n            containers = self.docker_client.containers.list(all=True)\n            \n            running_count = 0\n            stopped_count = 0\n            total_cpu = 0\n            total_memory = 0\n            \n            for container in containers:\n                if container.status == 'running':\n                    running_count += 1\n                else:\n                    stopped_count += 1\n            \n            return {\n                'status': 'healthy' if stopped_count == 0 else 'degraded',\n                'total_containers': len(containers),\n                'running_containers': running_count,\n                'stopped_containers': stopped_count\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to get container summary: {e}\")\n            return {'status': 'error', 'error': str(e)}\n    \n    def _get_current_storage_summary(self) -> Dict[str, Any]:\n        \"\"\"Get current storage usage summary\"\"\"\n        if not self.storage_metrics:\n            return {'status': 'no_data'}\n        \n        latest_metric = self.storage_metrics[-1]\n        filesystems = latest_metric['filesystems']\n        \n        total_space = sum(fs['total_bytes'] for fs in filesystems)\n        used_space = sum(fs['used_bytes'] for fs in filesystems)\n        free_space = sum(fs['free_bytes'] for fs in filesystems)\n        \n        overall_usage = (used_space / total_space) * 100 if total_space > 0 else 0\n        \n        # Determine status\n        status = 'healthy'\n        if overall_usage > self.thresholds['disk_critical']:\n            status = 'critical'\n        elif overall_usage > self.thresholds['disk_warning']:\n            status = 'warning'\n        \n        return {\n            'status': status,\n            'overall_usage_percent': overall_usage,\n            'total_space_gb': total_space / (1024**3),\n            'used_space_gb': used_space / (1024**3),\n            'free_space_gb': free_space / (1024**3),\n            'filesystems': [\n                {\n                    'device': fs['device'],\n                    'mountpoint': fs['mountpoint'],\n                    'usage_percent': fs['usage_percent'],\n                    'free_gb': fs['free_bytes'] / (1024**3)\n                }\n                for fs in filesystems\n            ]\n        }\n    \n    def get_alerts(self, severity: Optional[str] = None, limit: int = 50) -> List[Dict]:\n        \"\"\"Get infrastructure alerts\"\"\"\n        alerts = list(self._alerts)\n        \n        if severity:\n            alerts = [alert for alert in alerts if alert.get('severity') == severity]\n        \n        return sorted(alerts, key=lambda x: x['timestamp'], reverse=True)[:limit]\n\n\ndef init_infrastructure_monitoring(redis_client: Optional[redis.Redis] = None) -> InfrastructureMonitor:\n    \"\"\"Initialize infrastructure monitoring\"\"\"\n    monitor = InfrastructureMonitor(redis_client)\n    monitor.start_monitoring()\n    return monitor