"""
Advanced Alerting and Notification Service
Handles alert thresholds, escalation policies, and multi-channel notifications
"""

import time
import json
import logging
import threading
import smtplib
import requests
from datetime import datetime, timedelta
from collections import defaultdict, deque
from typing import Dict, Any, Optional, List, Callable
from dataclasses import dataclass, asdict
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from enum import Enum
import redis

from app.utils.logging import logger

class AlertSeverity(Enum):
    """Alert severity levels"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class AlertStatus(Enum):
    """Alert status"""
    OPEN = "open"
    ACKNOWLEDGED = "acknowledged"
    RESOLVED = "resolved"
    SUPPRESSED = "suppressed"

@dataclass
class Alert:
    """Alert data structure"""
    id: str
    name: str
    description: str
    severity: AlertSeverity
    status: AlertStatus
    source: str
    timestamp: float
    labels: Dict[str, str]
    annotations: Dict[str, str]
    fingerprint: str
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[float] = None
    resolved_at: Optional[float] = None
    escalation_level: int = 0
    notification_count: int = 0
    last_notification: Optional[float] = None

@dataclass
class NotificationChannel:
    """Notification channel configuration"""
    name: str
    type: str  # email, slack, webhook, sms
    config: Dict[str, Any]
    enabled: bool = True
    severity_filter: List[AlertSeverity] = None

@dataclass
class EscalationPolicy:
    """Alert escalation policy"""
    name: str
    levels: List[Dict[str, Any]]  # [{'duration_minutes': 15, 'channels': ['email']}, ...]
    enabled: bool = True
    severity_filter: List[AlertSeverity] = None

class AlertingService:
    """Comprehensive alerting and notification service"""
    
    def __init__(self, redis_client: Optional[redis.Redis] = None):
        self.redis_client = redis_client
        
        # Alert storage
        self.active_alerts = {}  # fingerprint -> Alert
        self.alert_history = deque(maxlen=10000)
        self.notification_queue = deque(maxlen=1000)
        
        # Configuration
        self.notification_channels = {}
        self.escalation_policies = {}
        self.alert_rules = {}
        self.suppression_rules = []
        
        # Grouping and throttling
        self.alert_groups = defaultdict(list)
        self.throttle_state = defaultdict(float)
        
        # Background processing
        self._processing_active = False\n        self._processing_thread = None\n        self._notification_thread = None\n        \n        # Default configuration\n        self._setup_default_config()\n        \n        logger.info(\"Alerting Service initialized\")\n    \n    def _setup_default_config(self):\n        \"\"\"Setup default alerting configuration\"\"\"\n        # Default notification channels\n        self.notification_channels = {\n            'email_admin': NotificationChannel(\n                name='admin_email',\n                type='email',\n                config={\n                    'recipients': ['admin@bdc.local'],\n                    'smtp_server': 'localhost',\n                    'smtp_port': 587,\n                    'username': '',\n                    'password': '',\n                    'use_tls': True\n                },\n                severity_filter=[AlertSeverity.HIGH, AlertSeverity.CRITICAL]\n            ),\n            'slack_alerts': NotificationChannel(\n                name='slack_alerts',\n                type='slack',\n                config={\n                    'webhook_url': '',\n                    'channel': '#alerts',\n                    'username': 'BDC Monitor'\n                },\n                severity_filter=[AlertSeverity.MEDIUM, AlertSeverity.HIGH, AlertSeverity.CRITICAL]\n            ),\n            'webhook_general': NotificationChannel(\n                name='webhook_general',\n                type='webhook',\n                config={\n                    'url': '',\n                    'headers': {'Content-Type': 'application/json'},\n                    'timeout': 10\n                }\n            )\n        }\n        \n        # Default escalation policies\n        self.escalation_policies = {\n            'critical_escalation': EscalationPolicy(\n                name='critical_escalation',\n                levels=[\n                    {'duration_minutes': 0, 'channels': ['email_admin', 'slack_alerts']},\n                    {'duration_minutes': 15, 'channels': ['email_admin', 'webhook_general']},\n                    {'duration_minutes': 30, 'channels': ['email_admin', 'slack_alerts']}\n                ],\n                severity_filter=[AlertSeverity.CRITICAL]\n            ),\n            'standard_escalation': EscalationPolicy(\n                name='standard_escalation',\n                levels=[\n                    {'duration_minutes': 0, 'channels': ['slack_alerts']},\n                    {'duration_minutes': 30, 'channels': ['email_admin']}\n                ],\n                severity_filter=[AlertSeverity.HIGH, AlertSeverity.MEDIUM]\n            )\n        }\n        \n        # Default alert rules with thresholds\n        self.alert_rules = {\n            'high_cpu': {\n                'name': 'High CPU Usage',\n                'description': 'CPU usage is above threshold',\n                'threshold': 80,\n                'duration_minutes': 5,\n                'severity': AlertSeverity.HIGH,\n                'labels': {'component': 'system', 'resource': 'cpu'}\n            },\n            'high_memory': {\n                'name': 'High Memory Usage',\n                'description': 'Memory usage is above threshold',\n                'threshold': 85,\n                'duration_minutes': 5,\n                'severity': AlertSeverity.HIGH,\n                'labels': {'component': 'system', 'resource': 'memory'}\n            },\n            'disk_space_critical': {\n                'name': 'Critical Disk Space',\n                'description': 'Disk space is critically low',\n                'threshold': 90,\n                'duration_minutes': 1,\n                'severity': AlertSeverity.CRITICAL,\n                'labels': {'component': 'system', 'resource': 'disk'}\n            },\n            'application_down': {\n                'name': 'Application Down',\n                'description': 'Application is not responding',\n                'threshold': 1,\n                'duration_minutes': 1,\n                'severity': AlertSeverity.CRITICAL,\n                'labels': {'component': 'application', 'type': 'availability'}\n            },\n            'high_error_rate': {\n                'name': 'High Error Rate',\n                'description': 'Application error rate is above threshold',\n                'threshold': 0.05,  # 5%\n                'duration_minutes': 10,\n                'severity': AlertSeverity.HIGH,\n                'labels': {'component': 'application', 'type': 'errors'}\n            },\n            'slow_response_time': {\n                'name': 'Slow Response Time',\n                'description': 'Application response time is above threshold',\n                'threshold': 2000,  # 2 seconds\n                'duration_minutes': 10,\n                'severity': AlertSeverity.MEDIUM,\n                'labels': {'component': 'application', 'type': 'performance'}\n            },\n            'database_slow_queries': {\n                'name': 'Database Slow Queries',\n                'description': 'Database queries are running slowly',\n                'threshold': 1000,  # 1 second\n                'duration_minutes': 5,\n                'severity': AlertSeverity.MEDIUM,\n                'labels': {'component': 'database', 'type': 'performance'}\n            },\n            'security_breach_attempt': {\n                'name': 'Security Breach Attempt',\n                'description': 'Potential security breach detected',\n                'threshold': 1,\n                'duration_minutes': 0,\n                'severity': AlertSeverity.CRITICAL,\n                'labels': {'component': 'security', 'type': 'breach'}\n            }\n        }\n    \n    def start_processing(self):\n        \"\"\"Start alert processing threads\"\"\"\n        if not self._processing_active:\n            self._processing_active = True\n            \n            # Start alert processing thread\n            self._processing_thread = threading.Thread(\n                target=self._processing_loop,\n                daemon=True\n            )\n            self._processing_thread.start()\n            \n            # Start notification processing thread\n            self._notification_thread = threading.Thread(\n                target=self._notification_loop,\n                daemon=True\n            )\n            self._notification_thread.start()\n            \n            logger.info(\"Alerting service processing started\")\n    \n    def stop_processing(self):\n        \"\"\"Stop alert processing\"\"\"\n        self._processing_active = False\n        \n        if self._processing_thread:\n            self._processing_thread.join(timeout=10)\n        \n        if self._notification_thread:\n            self._notification_thread.join(timeout=10)\n        \n        logger.info(\"Alerting service processing stopped\")\n    \n    def create_alert(self, \n                    alert_name: str,\n                    description: str,\n                    severity: AlertSeverity,\n                    source: str,\n                    labels: Optional[Dict[str, str]] = None,\n                    annotations: Optional[Dict[str, str]] = None) -> str:\n        \"\"\"Create a new alert\"\"\"\n        \n        labels = labels or {}\n        annotations = annotations or {}\n        \n        # Generate fingerprint for grouping\n        fingerprint = self._generate_fingerprint(alert_name, labels)\n        \n        # Check if alert already exists\n        if fingerprint in self.active_alerts:\n            existing_alert = self.active_alerts[fingerprint]\n            existing_alert.notification_count += 1\n            existing_alert.timestamp = time.time()\n            logger.debug(f\"Updated existing alert: {alert_name}\")\n            return existing_alert.id\n        \n        # Check suppression rules\n        if self._is_suppressed(alert_name, labels, severity):\n            logger.debug(f\"Alert suppressed: {alert_name}\")\n            return None\n        \n        # Create new alert\n        alert_id = f\"alert_{int(time.time() * 1000000)}\"\n        \n        alert = Alert(\n            id=alert_id,\n            name=alert_name,\n            description=description,\n            severity=severity,\n            status=AlertStatus.OPEN,\n            source=source,\n            timestamp=time.time(),\n            labels=labels,\n            annotations=annotations,\n            fingerprint=fingerprint\n        )\n        \n        # Store alert\n        self.active_alerts[fingerprint] = alert\n        self.alert_history.append(alert)\n        \n        # Store in Redis\n        if self.redis_client:\n            self._store_alert_redis(alert)\n        \n        # Queue for notification\n        self._queue_notification(alert)\n        \n        logger.info(f\"Created alert: {alert_name} ({severity.value}) from {source}\")\n        return alert_id\n    \n    def acknowledge_alert(self, alert_id: str, acknowledged_by: str) -> bool:\n        \"\"\"Acknowledge an alert\"\"\"\n        alert = self._find_alert_by_id(alert_id)\n        if not alert:\n            return False\n        \n        alert.status = AlertStatus.ACKNOWLEDGED\n        alert.acknowledged_by = acknowledged_by\n        alert.acknowledged_at = time.time()\n        \n        # Update in Redis\n        if self.redis_client:\n            self._store_alert_redis(alert)\n        \n        logger.info(f\"Alert {alert_id} acknowledged by {acknowledged_by}\")\n        return True\n    \n    def resolve_alert(self, alert_id: str) -> bool:\n        \"\"\"Resolve an alert\"\"\"\n        alert = self._find_alert_by_id(alert_id)\n        if not alert:\n            return False\n        \n        alert.status = AlertStatus.RESOLVED\n        alert.resolved_at = time.time()\n        \n        # Remove from active alerts\n        if alert.fingerprint in self.active_alerts:\n            del self.active_alerts[alert.fingerprint]\n        \n        # Update in Redis\n        if self.redis_client:\n            self._store_alert_redis(alert)\n        \n        logger.info(f\"Alert {alert_id} resolved\")\n        return True\n    \n    def resolve_alert_by_fingerprint(self, fingerprint: str) -> bool:\n        \"\"\"Resolve alert by fingerprint (for auto-resolution)\"\"\"\n        if fingerprint in self.active_alerts:\n            alert = self.active_alerts[fingerprint]\n            return self.resolve_alert(alert.id)\n        return False\n    \n    def _find_alert_by_id(self, alert_id: str) -> Optional[Alert]:\n        \"\"\"Find alert by ID\"\"\"\n        for alert in self.active_alerts.values():\n            if alert.id == alert_id:\n                return alert\n        return None\n    \n    def _generate_fingerprint(self, alert_name: str, labels: Dict[str, str]) -> str:\n        \"\"\"Generate unique fingerprint for alert grouping\"\"\"\n        import hashlib\n        \n        # Create a unique identifier based on alert name and key labels\n        key_labels = sorted(labels.items())\n        fingerprint_data = f\"{alert_name}:{':'.join(f'{k}={v}' for k, v in key_labels)}\"\n        \n        return hashlib.md5(fingerprint_data.encode()).hexdigest()\n    \n    def _is_suppressed(self, alert_name: str, labels: Dict[str, str], severity: AlertSeverity) -> bool:\n        \"\"\"Check if alert should be suppressed\"\"\"\n        for rule in self.suppression_rules:\n            if self._matches_suppression_rule(alert_name, labels, severity, rule):\n                return True\n        return False\n    \n    def _matches_suppression_rule(self, alert_name: str, labels: Dict[str, str], \n                                 severity: AlertSeverity, rule: Dict[str, Any]) -> bool:\n        \"\"\"Check if alert matches suppression rule\"\"\"\n        # Simple rule matching - can be extended\n        if 'alert_name' in rule and rule['alert_name'] != alert_name:\n            return False\n        \n        if 'severity' in rule and rule['severity'] != severity:\n            return False\n        \n        if 'labels' in rule:\n            for key, value in rule['labels'].items():\n                if labels.get(key) != value:\n                    return False\n        \n        return True\n    \n    def _queue_notification(self, alert: Alert):\n        \"\"\"Queue alert for notification\"\"\"\n        notification = {\n            'alert': alert,\n            'timestamp': time.time(),\n            'attempts': 0\n        }\n        \n        self.notification_queue.append(notification)\n    \n    def _processing_loop(self):\n        \"\"\"Main alert processing loop\"\"\"\n        while self._processing_active:\n            try:\n                # Process escalations\n                self._process_escalations()\n                \n                # Clean up resolved alerts\n                self._cleanup_old_alerts()\n                \n                # Update alert groups\n                self._update_alert_groups()\n                \n                time.sleep(30)  # Process every 30 seconds\n                \n            except Exception as e:\n                logger.error(f\"Error in alert processing loop: {e}\")\n                time.sleep(60)\n    \n    def _notification_loop(self):\n        \"\"\"Notification processing loop\"\"\"\n        while self._processing_active:\n            try:\n                # Process notification queue\n                if self.notification_queue:\n                    notification = self.notification_queue.popleft()\n                    self._process_notification(notification)\n                else:\n                    time.sleep(5)  # No notifications to process\n                    \n            except Exception as e:\n                logger.error(f\"Error in notification processing loop: {e}\")\n                time.sleep(10)\n    \n    def _process_escalations(self):\n        \"\"\"Process alert escalations\"\"\"\n        current_time = time.time()\n        \n        for alert in self.active_alerts.values():\n            if alert.status == AlertStatus.ACKNOWLEDGED:\n                continue\n            \n            # Find applicable escalation policy\n            policy = self._get_escalation_policy(alert)\n            if not policy:\n                continue\n            \n            # Check if escalation is needed\n            alert_age_minutes = (current_time - alert.timestamp) / 60\n            \n            for i, level in enumerate(policy.levels):\n                if (i > alert.escalation_level and \n                    alert_age_minutes >= level['duration_minutes']):\n                    \n                    # Escalate alert\n                    alert.escalation_level = i\n                    \n                    # Send escalation notifications\n                    for channel_name in level['channels']:\n                        if channel_name in self.notification_channels:\n                            self._send_notification(alert, channel_name, escalation=True)\n                    \n                    logger.warning(f\"Escalated alert {alert.id} to level {i}\")\n                    break\n    \n    def _get_escalation_policy(self, alert: Alert) -> Optional[EscalationPolicy]:\n        \"\"\"Get escalation policy for alert\"\"\"\n        for policy in self.escalation_policies.values():\n            if not policy.enabled:\n                continue\n            \n            if (policy.severity_filter and \n                alert.severity not in policy.severity_filter):\n                continue\n            \n            return policy\n        \n        return None\n    \n    def _process_notification(self, notification: Dict[str, Any]):\n        \"\"\"Process a single notification\"\"\"\n        alert = notification['alert']\n        \n        # Check throttling\n        if self._is_throttled(alert):\n            return\n        \n        # Find applicable notification channels\n        channels = self._get_notification_channels(alert)\n        \n        # Send notifications\n        for channel_name in channels:\n            try:\n                self._send_notification(alert, channel_name)\n                alert.last_notification = time.time()\n            except Exception as e:\n                logger.error(f\"Failed to send notification via {channel_name}: {e}\")\n                notification['attempts'] += 1\n                \n                # Retry if not too many attempts\n                if notification['attempts'] < 3:\n                    self.notification_queue.append(notification)\n    \n    def _is_throttled(self, alert: Alert) -> bool:\n        \"\"\"Check if alert notifications should be throttled\"\"\"\n        throttle_key = f\"{alert.fingerprint}:{alert.severity.value}\"\n        last_notification = self.throttle_state.get(throttle_key, 0)\n        \n        # Throttle based on severity\n        throttle_intervals = {\n            AlertSeverity.LOW: 3600,      # 1 hour\n            AlertSeverity.MEDIUM: 1800,   # 30 minutes\n            AlertSeverity.HIGH: 900,      # 15 minutes\n            AlertSeverity.CRITICAL: 300   # 5 minutes\n        }\n        \n        throttle_interval = throttle_intervals.get(alert.severity, 900)\n        \n        if time.time() - last_notification < throttle_interval:\n            return True\n        \n        self.throttle_state[throttle_key] = time.time()\n        return False\n    \n    def _get_notification_channels(self, alert: Alert) -> List[str]:\n        \"\"\"Get notification channels for alert\"\"\"\n        channels = []\n        \n        for channel_name, channel in self.notification_channels.items():\n            if not channel.enabled:\n                continue\n            \n            if (channel.severity_filter and \n                alert.severity not in channel.severity_filter):\n                continue\n            \n            channels.append(channel_name)\n        \n        return channels\n    \n    def _send_notification(self, alert: Alert, channel_name: str, escalation: bool = False):\n        \"\"\"Send notification via specified channel\"\"\"\n        channel = self.notification_channels.get(channel_name)\n        if not channel:\n            return\n        \n        try:\n            if channel.type == 'email':\n                self._send_email_notification(alert, channel, escalation)\n            elif channel.type == 'slack':\n                self._send_slack_notification(alert, channel, escalation)\n            elif channel.type == 'webhook':\n                self._send_webhook_notification(alert, channel, escalation)\n            elif channel.type == 'sms':\n                self._send_sms_notification(alert, channel, escalation)\n            else:\n                logger.warning(f\"Unknown notification channel type: {channel.type}\")\n        \n        except Exception as e:\n            logger.error(f\"Failed to send {channel.type} notification: {e}\")\n            raise\n    \n    def _send_email_notification(self, alert: Alert, channel: NotificationChannel, escalation: bool):\n        \"\"\"Send email notification\"\"\"\n        config = channel.config\n        \n        # Create email message\n        msg = MIMEMultipart()\n        msg['From'] = config.get('from_email', 'alerts@bdc.local')\n        msg['To'] = ', '.join(config['recipients'])\n        \n        escalation_prefix = \"[ESCALATED] \" if escalation else \"\"\n        msg['Subject'] = f\"{escalation_prefix}[BDC {alert.severity.value.upper()}] {alert.name}\"\n        \n        # Create email body\n        body = self._create_email_body(alert, escalation)\n        msg.attach(MIMEText(body, 'html'))\n        \n        # Send email\n        try:\n            with smtplib.SMTP(config['smtp_server'], config['smtp_port']) as server:\n                if config.get('use_tls', True):\n                    server.starttls()\n                \n                if config.get('username') and config.get('password'):\n                    server.login(config['username'], config['password'])\n                \n                server.send_message(msg)\n                \n            logger.info(f\"Email notification sent for alert {alert.id}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to send email notification: {e}\")\n            raise\n    \n    def _send_slack_notification(self, alert: Alert, channel: NotificationChannel, escalation: bool):\n        \"\"\"Send Slack notification\"\"\"\n        config = channel.config\n        webhook_url = config.get('webhook_url')\n        \n        if not webhook_url:\n            logger.error(\"Slack webhook URL not configured\")\n            return\n        \n        # Create Slack message\n        escalation_prefix = \"🚨 ESCALATED: \" if escalation else \"\"\n        \n        color_map = {\n            AlertSeverity.LOW: \"good\",\n            AlertSeverity.MEDIUM: \"warning\", \n            AlertSeverity.HIGH: \"warning\",\n            AlertSeverity.CRITICAL: \"danger\"\n        }\n        \n        payload = {\n            \"channel\": config.get('channel', '#alerts'),\n            \"username\": config.get('username', 'BDC Monitor'),\n            \"text\": f\"{escalation_prefix}{alert.name}\",\n            \"attachments\": [\n                {\n                    \"color\": color_map.get(alert.severity, \"warning\"),\n                    \"fields\": [\n                        {\n                            \"title\": \"Severity\",\n                            \"value\": alert.severity.value.upper(),\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Source\",\n                            \"value\": alert.source,\n                            \"short\": True\n                        },\n                        {\n                            \"title\": \"Description\",\n                            \"value\": alert.description,\n                            \"short\": False\n                        }\n                    ],\n                    \"ts\": int(alert.timestamp)\n                }\n            ]\n        }\n        \n        # Add labels and annotations\n        if alert.labels:\n            payload[\"attachments\"][0][\"fields\"].append({\n                \"title\": \"Labels\",\n                \"value\": \", \".join(f\"{k}={v}\" for k, v in alert.labels.items()),\n                \"short\": False\n            })\n        \n        # Send Slack message\n        response = requests.post(webhook_url, json=payload, timeout=10)\n        response.raise_for_status()\n        \n        logger.info(f\"Slack notification sent for alert {alert.id}\")\n    \n    def _send_webhook_notification(self, alert: Alert, channel: NotificationChannel, escalation: bool):\n        \"\"\"Send webhook notification\"\"\"\n        config = channel.config\n        url = config.get('url')\n        \n        if not url:\n            logger.error(\"Webhook URL not configured\")\n            return\n        \n        # Create webhook payload\n        payload = {\n            \"alert\": asdict(alert),\n            \"escalation\": escalation,\n            \"timestamp\": time.time()\n        }\n        \n        # Convert enums to strings for JSON serialization\n        payload[\"alert\"][\"severity\"] = alert.severity.value\n        payload[\"alert\"][\"status\"] = alert.status.value\n        \n        headers = config.get('headers', {'Content-Type': 'application/json'})\n        timeout = config.get('timeout', 10)\n        \n        # Send webhook\n        response = requests.post(url, json=payload, headers=headers, timeout=timeout)\n        response.raise_for_status()\n        \n        logger.info(f\"Webhook notification sent for alert {alert.id}\")\n    \n    def _send_sms_notification(self, alert: Alert, channel: NotificationChannel, escalation: bool):\n        \"\"\"Send SMS notification (placeholder - integrate with SMS provider)\"\"\"\n        config = channel.config\n        \n        # This would integrate with SMS providers like Twilio, AWS SNS, etc.\n        logger.info(f\"SMS notification would be sent for alert {alert.id}\")\n        # Implementation depends on SMS provider\n    \n    def _create_email_body(self, alert: Alert, escalation: bool) -> str:\n        \"\"\"Create HTML email body for alert\"\"\"\n        escalation_notice = \"\"\n        if escalation:\n            escalation_notice = \"<p style='color: red; font-weight: bold;'>🚨 This alert has been escalated due to lack of acknowledgment.</p>\"\n        \n        labels_html = \"\"\n        if alert.labels:\n            labels_html = \"<h3>Labels:</h3><ul>\" + \\\n                         \"\".join(f\"<li><strong>{k}:</strong> {v}</li>\" for k, v in alert.labels.items()) + \\\n                         \"</ul>\"\n        \n        annotations_html = \"\"\n        if alert.annotations:\n            annotations_html = \"<h3>Annotations:</h3><ul>\" + \\\n                              \"\".join(f\"<li><strong>{k}:</strong> {v}</li>\" for k, v in alert.annotations.items()) + \\\n                              \"</ul>\"\n        \n        return f\"\"\"\n        <html>\n            <body>\n                {escalation_notice}\n                <h2>Alert: {alert.name}</h2>\n                <p><strong>Severity:</strong> {alert.severity.value.upper()}</p>\n                <p><strong>Source:</strong> {alert.source}</p>\n                <p><strong>Description:</strong> {alert.description}</p>\n                <p><strong>Time:</strong> {datetime.fromtimestamp(alert.timestamp).strftime('%Y-%m-%d %H:%M:%S')}</p>\n                {labels_html}\n                {annotations_html}\n                <hr>\n                <p><em>Alert ID: {alert.id}</em></p>\n                <p><em>Fingerprint: {alert.fingerprint}</em></p>\n            </body>\n        </html>\n        \"\"\"\n    \n    def _cleanup_old_alerts(self):\n        \"\"\"Clean up old resolved alerts\"\"\"\n        cutoff_time = time.time() - (7 * 24 * 3600)  # 7 days\n        \n        # Clean up alert history\n        self.alert_history = deque(\n            (alert for alert in self.alert_history \n             if alert.timestamp > cutoff_time or alert.status != AlertStatus.RESOLVED),\n            maxlen=10000\n        )\n    \n    def _update_alert_groups(self):\n        \"\"\"Update alert grouping\"\"\"\n        # Group alerts by labels for dashboard display\n        self.alert_groups.clear()\n        \n        for alert in self.active_alerts.values():\n            group_key = alert.labels.get('component', 'unknown')\n            self.alert_groups[group_key].append(alert)\n    \n    def _store_alert_redis(self, alert: Alert):\n        \"\"\"Store alert in Redis\"\"\"\n        try:\n            alert_data = asdict(alert)\n            alert_data['severity'] = alert.severity.value\n            alert_data['status'] = alert.status.value\n            \n            # Store active alert\n            if alert.status in [AlertStatus.OPEN, AlertStatus.ACKNOWLEDGED]:\n                self.redis_client.hset(\n                    \"alerts:active\",\n                    alert.fingerprint,\n                    json.dumps(alert_data)\n                )\n            else:\n                # Remove from active alerts\n                self.redis_client.hdel(\"alerts:active\", alert.fingerprint)\n            \n            # Store in history\n            self.redis_client.lpush(\n                \"alerts:history\",\n                json.dumps(alert_data)\n            )\n            \n            # Keep only last 10000 in history\n            self.redis_client.ltrim(\"alerts:history\", 0, 9999)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store alert in Redis: {e}\")\n    \n    def get_active_alerts(self, severity: Optional[AlertSeverity] = None) -> List[Alert]:\n        \"\"\"Get active alerts\"\"\"\n        alerts = list(self.active_alerts.values())\n        \n        if severity:\n            alerts = [alert for alert in alerts if alert.severity == severity]\n        \n        return sorted(alerts, key=lambda x: x.timestamp, reverse=True)\n    \n    def get_alert_summary(self) -> Dict[str, Any]:\n        \"\"\"Get alert summary statistics\"\"\"\n        active_alerts = list(self.active_alerts.values())\n        \n        severity_counts = defaultdict(int)\n        status_counts = defaultdict(int)\n        source_counts = defaultdict(int)\n        \n        for alert in active_alerts:\n            severity_counts[alert.severity.value] += 1\n            status_counts[alert.status.value] += 1\n            source_counts[alert.source] += 1\n        \n        return {\n            'timestamp': datetime.utcnow().isoformat(),\n            'total_active_alerts': len(active_alerts),\n            'severity_breakdown': dict(severity_counts),\n            'status_breakdown': dict(status_counts),\n            'source_breakdown': dict(source_counts),\n            'alert_groups': {k: len(v) for k, v in self.alert_groups.items()},\n            'recent_alerts': [asdict(alert) for alert in active_alerts[-5:]]\n        }\n    \n    def add_suppression_rule(self, rule: Dict[str, Any]):\n        \"\"\"Add alert suppression rule\"\"\"\n        self.suppression_rules.append(rule)\n        logger.info(f\"Added suppression rule: {rule}\")\n    \n    def remove_suppression_rule(self, rule_index: int) -> bool:\n        \"\"\"Remove suppression rule by index\"\"\"\n        if 0 <= rule_index < len(self.suppression_rules):\n            removed_rule = self.suppression_rules.pop(rule_index)\n            logger.info(f\"Removed suppression rule: {removed_rule}\")\n            return True\n        return False\n    \n    def update_notification_channel(self, channel_name: str, channel: NotificationChannel):\n        \"\"\"Update notification channel configuration\"\"\"\n        self.notification_channels[channel_name] = channel\n        logger.info(f\"Updated notification channel: {channel_name}\")\n    \n    def update_escalation_policy(self, policy_name: str, policy: EscalationPolicy):\n        \"\"\"Update escalation policy\"\"\"\n        self.escalation_policies[policy_name] = policy\n        logger.info(f\"Updated escalation policy: {policy_name}\")\n\n\ndef init_alerting_service(redis_client: Optional[redis.Redis] = None) -> AlertingService:\n    \"\"\"Initialize alerting service\"\"\"\n    service = AlertingService(redis_client)\n    service.start_processing()\n    return service