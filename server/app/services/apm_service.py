"""
Advanced Application Performance Monitoring (APM) Service
Provides comprehensive monitoring, tracing, and performance analytics
"""

import time
import json
import logging
import threading
from datetime import datetime, timedelta
from collections import defaultdict, deque
from contextlib import contextmanager
from typing import Dict, Any, Optional, List, Callable
import psutil
import redis
import numpy as np
from flask import Flask, request, g, current_app
from sqlalchemy import text, event
from sqlalchemy.engine import Engine

from app.extensions import db
from app.utils.logging import logger

class APMService:
    """Comprehensive Application Performance Monitoring"""
    
    def __init__(self, app: Optional[Flask] = None, redis_client: Optional[redis.Redis] = None):
        self.app = app
        self.redis_client = redis_client
        
        # Performance data storage
        self.request_metrics = deque(maxlen=10000)
        self.database_metrics = deque(maxlen=5000)
        self.background_job_metrics = deque(maxlen=1000)
        self.error_metrics = deque(maxlen=1000)
        
        # Real-time aggregations
        self.endpoint_stats = defaultdict(lambda: {
            'count': 0,
            'total_time': 0,
            'error_count': 0,
            'slow_requests': 0,
            'response_times': deque(maxlen=100)
        })
        
        self.database_stats = {
            'query_count': 0,
            'total_query_time': 0,
            'slow_queries': deque(maxlen=100),
            'connection_pool_stats': {}
        }
        
        # Performance thresholds
        self.thresholds = {
            'slow_request_ms': 1000,
            'slow_query_ms': 500,
            'high_cpu_percent': 80,
            'high_memory_percent': 85,
            'high_error_rate': 0.05,
            'connection_pool_warning': 0.8
        }
        
        # Background monitoring
        self._monitoring_active = False
        self._monitoring_thread = None
        self._alerts_queue = deque(maxlen=100)
        
        if app:
            self.init_app(app, redis_client)
    
    def init_app(self, app: Flask, redis_client: Optional[redis.Redis] = None):
        """Initialize with Flask application"""
        self.app = app
        self.redis_client = redis_client or app.extensions.get('redis')
        
        # Register Flask hooks
        app.before_request(self._before_request)
        app.after_request(self._after_request)
        app.teardown_appcontext(self._teardown_request)
        
        # Register database event listeners
        self._setup_database_monitoring()
        
        # Start background monitoring
        self.start_monitoring()
        
        # Store reference in app
        app.apm_service = self
        
        logger.info("APM Service initialized successfully")
    
    def _before_request(self):
        """Track request start and context"""
        g.apm_start_time = time.time()
        g.apm_context = {
            'endpoint': request.endpoint,
            'method': request.method,
            'path': request.path,
            'remote_addr': request.remote_addr,
            'user_agent': request.headers.get('User-Agent', ''),
            'content_length': request.content_length or 0,
            'args': dict(request.args),
            'db_queries': 0,
            'db_query_time': 0
        }
    
    def _after_request(self, response):
        """Track request completion and metrics"""
        if hasattr(g, 'apm_start_time'):
            end_time = time.time()
            duration_ms = (end_time - g.apm_start_time) * 1000
            
            # Update context with response data
            g.apm_context.update({
                'status_code': response.status_code,
                'response_size': len(response.get_data()),
                'duration_ms': duration_ms,
                'end_time': end_time,
                'successful': 200 <= response.status_code < 400
            })\n            \n            # Store request metrics\n            self._record_request_metrics(g.apm_context)\n            \n            # Check for performance issues\n            self._check_request_performance(g.apm_context)\n        \n        return response\n    \n    def _teardown_request(self, exception=None):\n        \"\"\"Handle request teardown and errors\"\"\"\n        if exception and hasattr(g, 'apm_context'):\n            self._record_error(exception, g.apm_context)\n    \n    def _setup_database_monitoring(self):\n        \"\"\"Setup SQLAlchemy event listeners for database monitoring\"\"\"\n        \n        @event.listens_for(Engine, \"before_cursor_execute\")\n        def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n            context._query_start_time = time.time()\n        \n        @event.listens_for(Engine, \"after_cursor_execute\")\n        def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n            if hasattr(context, '_query_start_time'):\n                query_time = (time.time() - context._query_start_time) * 1000\n                \n                # Update request context if available\n                if hasattr(g, 'apm_context'):\n                    g.apm_context['db_queries'] += 1\n                    g.apm_context['db_query_time'] += query_time\n                \n                # Record database metrics\n                self._record_database_metrics({\n                    'query': statement[:200],  # Truncate long queries\n                    'duration_ms': query_time,\n                    'timestamp': time.time(),\n                    'parameters_count': len(parameters) if parameters else 0\n                })\n                \n                # Check for slow queries\n                if query_time > self.thresholds['slow_query_ms']:\n                    self._record_slow_query(statement, query_time, parameters)\n    \n    def _record_request_metrics(self, context: Dict[str, Any]):\n        \"\"\"Record request performance metrics\"\"\"\n        # Add to metrics storage\n        self.request_metrics.append(context.copy())\n        \n        # Update endpoint statistics\n        endpoint = context.get('endpoint', 'unknown')\n        stats = self.endpoint_stats[endpoint]\n        \n        stats['count'] += 1\n        stats['total_time'] += context['duration_ms']\n        stats['response_times'].append(context['duration_ms'])\n        \n        if not context['successful']:\n            stats['error_count'] += 1\n        \n        if context['duration_ms'] > self.thresholds['slow_request_ms']:\n            stats['slow_requests'] += 1\n        \n        # Store in Redis for distributed metrics\n        if self.redis_client:\n            self._store_redis_request_metrics(context)\n    \n    def _record_database_metrics(self, query_data: Dict[str, Any]):\n        \"\"\"Record database query metrics\"\"\"\n        self.database_metrics.append(query_data)\n        \n        # Update database statistics\n        self.database_stats['query_count'] += 1\n        self.database_stats['total_query_time'] += query_data['duration_ms']\n        \n        # Store in Redis\n        if self.redis_client:\n            self._store_redis_db_metrics(query_data)\n    \n    def _record_slow_query(self, query: str, duration_ms: float, parameters: Any):\n        \"\"\"Record slow query for analysis\"\"\"\n        slow_query = {\n            'query': query[:500],  # Truncate for storage\n            'duration_ms': duration_ms,\n            'timestamp': time.time(),\n            'parameters': str(parameters)[:200] if parameters else None\n        }\n        \n        self.database_stats['slow_queries'].append(slow_query)\n        \n        # Alert for very slow queries\n        if duration_ms > self.thresholds['slow_query_ms'] * 5:  # 5x threshold\n            self._create_alert('slow_query', {\n                'query': query[:100],\n                'duration_ms': duration_ms,\n                'severity': 'high'\n            })\n        \n        logger.warning(f\"Slow query detected: {duration_ms:.2f}ms - {query[:100]}\")\n    \n    def _record_error(self, exception: Exception, context: Dict[str, Any]):\n        \"\"\"Record application errors with context\"\"\"\n        error_data = {\n            'exception_type': type(exception).__name__,\n            'exception_message': str(exception),\n            'endpoint': context.get('endpoint'),\n            'method': context.get('method'),\n            'path': context.get('path'),\n            'timestamp': time.time(),\n            'user_agent': context.get('user_agent'),\n            'remote_addr': context.get('remote_addr')\n        }\n        \n        self.error_metrics.append(error_data)\n        \n        # Create alert for errors\n        self._create_alert('application_error', {\n            'exception': type(exception).__name__,\n            'endpoint': context.get('endpoint'),\n            'severity': 'medium'\n        })\n        \n        # Store in Redis\n        if self.redis_client:\n            self._store_redis_error(error_data)\n    \n    def _check_request_performance(self, context: Dict[str, Any]):\n        \"\"\"Check request performance against thresholds\"\"\"\n        issues = []\n        \n        # Check response time\n        if context['duration_ms'] > self.thresholds['slow_request_ms']:\n            issues.append({\n                'type': 'slow_request',\n                'value': context['duration_ms'],\n                'threshold': self.thresholds['slow_request_ms']\n            })\n        \n        # Check database query performance\n        if context.get('db_query_time', 0) > self.thresholds['slow_query_ms']:\n            issues.append({\n                'type': 'slow_database',\n                'value': context['db_query_time'],\n                'threshold': self.thresholds['slow_query_ms']\n            })\n        \n        # Check error rate\n        endpoint = context.get('endpoint', 'unknown')\n        stats = self.endpoint_stats[endpoint]\n        if stats['count'] > 10:  # Only check after minimum requests\n            error_rate = stats['error_count'] / stats['count']\n            if error_rate > self.thresholds['high_error_rate']:\n                issues.append({\n                    'type': 'high_error_rate',\n                    'value': error_rate,\n                    'threshold': self.thresholds['high_error_rate']\n                })\n        \n        # Create alerts for issues\n        for issue in issues:\n            self._create_alert(issue['type'], {\n                'endpoint': endpoint,\n                'value': issue['value'],\n                'threshold': issue['threshold'],\n                'severity': 'medium'\n            })\n    \n    def _store_redis_request_metrics(self, context: Dict[str, Any]):\n        \"\"\"Store request metrics in Redis\"\"\"\n        try:\n            # Create time-based key\n            time_key = datetime.now().strftime('%Y%m%d%H')\n            \n            # Store individual request\n            request_data = {\n                'endpoint': context.get('endpoint'),\n                'method': context.get('method'),\n                'duration_ms': context['duration_ms'],\n                'status_code': context['status_code'],\n                'timestamp': context['end_time']\n            }\n            \n            self.redis_client.lpush(\n                f\"apm:requests:{time_key}\",\n                json.dumps(request_data)\n            )\n            \n            # Set expiry (24 hours)\n            self.redis_client.expire(f\"apm:requests:{time_key}\", 86400)\n            \n            # Update counters\n            endpoint = context.get('endpoint', 'unknown')\n            self.redis_client.hincrby(f\"apm:endpoint_counts:{time_key}\", endpoint, 1)\n            self.redis_client.hincrbyfloat(\n                f\"apm:endpoint_times:{time_key}\",\n                endpoint,\n                context['duration_ms']\n            )\n            \n            if not context['successful']:\n                self.redis_client.hincrby(f\"apm:endpoint_errors:{time_key}\", endpoint, 1)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store request metrics in Redis: {e}\")\n    \n    def _store_redis_db_metrics(self, query_data: Dict[str, Any]):\n        \"\"\"Store database metrics in Redis\"\"\"\n        try:\n            time_key = datetime.now().strftime('%Y%m%d%H')\n            \n            # Store query metrics\n            self.redis_client.lpush(\n                f\"apm:db_queries:{time_key}\",\n                json.dumps({\n                    'duration_ms': query_data['duration_ms'],\n                    'timestamp': query_data['timestamp']\n                })\n            )\n            \n            # Set expiry\n            self.redis_client.expire(f\"apm:db_queries:{time_key}\", 86400)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store DB metrics in Redis: {e}\")\n    \n    def _store_redis_error(self, error_data: Dict[str, Any]):\n        \"\"\"Store error metrics in Redis\"\"\"\n        try:\n            self.redis_client.lpush(\n                \"apm:errors\",\n                json.dumps(error_data)\n            )\n            \n            # Keep only last 1000 errors\n            self.redis_client.ltrim(\"apm:errors\", 0, 999)\n            \n        except Exception as e:\n            logger.error(f\"Failed to store error in Redis: {e}\")\n    \n    def _create_alert(self, alert_type: str, data: Dict[str, Any]):\n        \"\"\"Create performance alert\"\"\"\n        alert = {\n            'type': alert_type,\n            'timestamp': time.time(),\n            'data': data,\n            'severity': data.get('severity', 'medium')\n        }\n        \n        self._alerts_queue.append(alert)\n        \n        # Store in Redis for alert manager\n        if self.redis_client:\n            self.redis_client.lpush(\"apm:alerts\", json.dumps(alert))\n            self.redis_client.ltrim(\"apm:alerts\", 0, 99)\n    \n    @contextmanager\n    def trace_operation(self, operation_name: str, metadata: Optional[Dict] = None):\n        \"\"\"Context manager for tracing custom operations\"\"\"\n        start_time = time.time()\n        \n        try:\n            yield\n        except Exception as e:\n            # Record operation error\n            self._record_operation_error(operation_name, e, metadata)\n            raise\n        finally:\n            # Record operation metrics\n            duration_ms = (time.time() - start_time) * 1000\n            self._record_operation_metrics(operation_name, duration_ms, metadata)\n    \n    def _record_operation_metrics(self, operation: str, duration_ms: float, metadata: Optional[Dict]):\n        \"\"\"Record custom operation metrics\"\"\"\n        operation_data = {\n            'operation': operation,\n            'duration_ms': duration_ms,\n            'timestamp': time.time(),\n            'metadata': metadata or {}\n        }\n        \n        self.background_job_metrics.append(operation_data)\n        \n        # Store in Redis\n        if self.redis_client:\n            self.redis_client.lpush(\n                \"apm:operations\",\n                json.dumps(operation_data)\n            )\n            self.redis_client.ltrim(\"apm:operations\", 0, 999)\n    \n    def _record_operation_error(self, operation: str, exception: Exception, metadata: Optional[Dict]):\n        \"\"\"Record operation error\"\"\"\n        self._create_alert('operation_error', {\n            'operation': operation,\n            'exception': type(exception).__name__,\n            'message': str(exception),\n            'metadata': metadata or {},\n            'severity': 'high'\n        })\n    \n    def get_performance_summary(self, hours: int = 1) -> Dict[str, Any]:\n        \"\"\"Get comprehensive performance summary\"\"\"\n        cutoff_time = time.time() - (hours * 3600)\n        \n        # Filter recent metrics\n        recent_requests = [\n            req for req in self.request_metrics\n            if req.get('end_time', 0) > cutoff_time\n        ]\n        \n        recent_db_queries = [\n            query for query in self.database_metrics\n            if query.get('timestamp', 0) > cutoff_time\n        ]\n        \n        recent_errors = [\n            error for error in self.error_metrics\n            if error.get('timestamp', 0) > cutoff_time\n        ]\n        \n        # Calculate request metrics\n        request_summary = self._calculate_request_summary(recent_requests)\n        db_summary = self._calculate_db_summary(recent_db_queries)\n        error_summary = self._calculate_error_summary(recent_errors)\n        \n        # System metrics\n        system_summary = self._get_system_summary()\n        \n        return {\n            'timestamp': datetime.utcnow().isoformat(),\n            'period_hours': hours,\n            'requests': request_summary,\n            'database': db_summary,\n            'errors': error_summary,\n            'system': system_summary,\n            'alerts': list(self._alerts_queue)[-10:],  # Last 10 alerts\n            'thresholds': self.thresholds\n        }\n    \n    def _calculate_request_summary(self, requests: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Calculate request performance summary\"\"\"\n        if not requests:\n            return {'total': 0, 'average_response_time': 0, 'error_rate': 0}\n        \n        total_requests = len(requests)\n        successful_requests = sum(1 for req in requests if req.get('successful', False))\n        total_response_time = sum(req.get('duration_ms', 0) for req in requests)\n        \n        response_times = [req.get('duration_ms', 0) for req in requests]\n        \n        return {\n            'total': total_requests,\n            'successful': successful_requests,\n            'failed': total_requests - successful_requests,\n            'error_rate': (total_requests - successful_requests) / total_requests,\n            'average_response_time': total_response_time / total_requests,\n            'percentiles': {\n                'p50': np.percentile(response_times, 50),\n                'p75': np.percentile(response_times, 75),\n                'p90': np.percentile(response_times, 90),\n                'p95': np.percentile(response_times, 95),\n                'p99': np.percentile(response_times, 99)\n            },\n            'slowest_endpoints': self._get_slowest_endpoints(requests)\n        }\n    \n    def _calculate_db_summary(self, queries: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Calculate database performance summary\"\"\"\n        if not queries:\n            return {'total_queries': 0, 'average_query_time': 0}\n        \n        total_queries = len(queries)\n        total_query_time = sum(query.get('duration_ms', 0) for query in queries)\n        \n        query_times = [query.get('duration_ms', 0) for query in queries]\n        slow_queries = [q for q in queries if q.get('duration_ms', 0) > self.thresholds['slow_query_ms']]\n        \n        return {\n            'total_queries': total_queries,\n            'average_query_time': total_query_time / total_queries,\n            'slow_queries_count': len(slow_queries),\n            'slow_queries_rate': len(slow_queries) / total_queries,\n            'percentiles': {\n                'p50': np.percentile(query_times, 50),\n                'p75': np.percentile(query_times, 75),\n                'p90': np.percentile(query_times, 90),\n                'p95': np.percentile(query_times, 95),\n                'p99': np.percentile(query_times, 99)\n            }\n        }\n    \n    def _calculate_error_summary(self, errors: List[Dict]) -> Dict[str, Any]:\n        \"\"\"Calculate error summary\"\"\"\n        if not errors:\n            return {'total_errors': 0, 'error_types': {}}\n        \n        error_types = defaultdict(int)\n        for error in errors:\n            error_types[error.get('exception_type', 'Unknown')] += 1\n        \n        return {\n            'total_errors': len(errors),\n            'error_types': dict(error_types),\n            'recent_errors': errors[-5:]  # Last 5 errors\n        }\n    \n    def _get_system_summary(self) -> Dict[str, Any]:\n        \"\"\"Get current system performance summary\"\"\"\n        try:\n            return {\n                'cpu_percent': psutil.cpu_percent(interval=1),\n                'memory_percent': psutil.virtual_memory().percent,\n                'disk_percent': psutil.disk_usage('/').percent,\n                'network_connections': len(psutil.net_connections()),\n                'process_count': len(psutil.pids())\n            }\n        except Exception as e:\n            logger.error(f\"Failed to get system metrics: {e}\")\n            return {}\n    \n    def _get_slowest_endpoints(self, requests: List[Dict], limit: int = 5) -> List[Dict]:\n        \"\"\"Get slowest endpoints from recent requests\"\"\"\n        endpoint_times = defaultdict(list)\n        \n        for req in requests:\n            endpoint = req.get('endpoint', 'unknown')\n            endpoint_times[endpoint].append(req.get('duration_ms', 0))\n        \n        slowest = []\n        for endpoint, times in endpoint_times.items():\n            avg_time = sum(times) / len(times)\n            slowest.append({\n                'endpoint': endpoint,\n                'average_time': avg_time,\n                'request_count': len(times),\n                'max_time': max(times)\n            })\n        \n        return sorted(slowest, key=lambda x: x['average_time'], reverse=True)[:limit]\n    \n    def start_monitoring(self):\n        \"\"\"Start background monitoring thread\"\"\"\n        if not self._monitoring_active:\n            self._monitoring_active = True\n            self._monitoring_thread = threading.Thread(\n                target=self._monitoring_loop,\n                daemon=True\n            )\n            self._monitoring_thread.start()\n            logger.info(\"APM background monitoring started\")\n    \n    def stop_monitoring(self):\n        \"\"\"Stop background monitoring\"\"\"\n        self._monitoring_active = False\n        if self._monitoring_thread:\n            self._monitoring_thread.join(timeout=5)\n            logger.info(\"APM background monitoring stopped\")\n    \n    def _monitoring_loop(self):\n        \"\"\"Background monitoring loop\"\"\"\n        while self._monitoring_active:\n            try:\n                # System performance checks\n                self._check_system_performance()\n                \n                # Database connection pool checks\n                self._check_database_health()\n                \n                # Clean old metrics\n                self._cleanup_old_metrics()\n                \n                # Sleep for 30 seconds\n                time.sleep(30)\n                \n            except Exception as e:\n                logger.error(f\"Error in APM monitoring loop: {e}\")\n                time.sleep(60)  # Wait longer on error\n    \n    def _check_system_performance(self):\n        \"\"\"Check system performance thresholds\"\"\"\n        try:\n            cpu_percent = psutil.cpu_percent(interval=1)\n            memory_percent = psutil.virtual_memory().percent\n            \n            if cpu_percent > self.thresholds['high_cpu_percent']:\n                self._create_alert('high_cpu', {\n                    'cpu_percent': cpu_percent,\n                    'threshold': self.thresholds['high_cpu_percent'],\n                    'severity': 'high' if cpu_percent > 90 else 'medium'\n                })\n            \n            if memory_percent > self.thresholds['high_memory_percent']:\n                self._create_alert('high_memory', {\n                    'memory_percent': memory_percent,\n                    'threshold': self.thresholds['high_memory_percent'],\n                    'severity': 'high' if memory_percent > 95 else 'medium'\n                })\n                \n        except Exception as e:\n            logger.error(f\"Failed to check system performance: {e}\")\n    \n    def _check_database_health(self):\n        \"\"\"Check database connection pool health\"\"\"\n        try:\n            # Get connection pool info\n            engine = db.get_engine()\n            pool = engine.pool\n            \n            pool_status = {\n                'size': pool.size(),\n                'checked_in': pool.checkedin(),\n                'checked_out': pool.checkedout(),\n                'overflow': pool.overflow(),\n                'invalidated': pool.invalidated()\n            }\n            \n            # Calculate pool utilization\n            total_connections = pool_status['size'] + pool_status['overflow']\n            if total_connections > 0:\n                utilization = pool_status['checked_out'] / total_connections\n                \n                if utilization > self.thresholds['connection_pool_warning']:\n                    self._create_alert('high_db_connections', {\n                        'utilization': utilization,\n                        'checked_out': pool_status['checked_out'],\n                        'total': total_connections,\n                        'severity': 'high' if utilization > 0.95 else 'medium'\n                    })\n            \n            # Store pool stats\n            self.database_stats['connection_pool_stats'] = pool_status\n            \n        except Exception as e:\n            logger.error(f\"Failed to check database health: {e}\")\n    \n    def _cleanup_old_metrics(self):\n        \"\"\"Clean up old metrics to prevent memory issues\"\"\"\n        cutoff_time = time.time() - (24 * 3600)  # 24 hours\n        \n        # Clean request metrics\n        self.request_metrics = deque(\n            (req for req in self.request_metrics \n             if req.get('end_time', 0) > cutoff_time),\n            maxlen=10000\n        )\n        \n        # Clean database metrics\n        self.database_metrics = deque(\n            (query for query in self.database_metrics \n             if query.get('timestamp', 0) > cutoff_time),\n            maxlen=5000\n        )\n        \n        # Clean error metrics\n        self.error_metrics = deque(\n            (error for error in self.error_metrics \n             if error.get('timestamp', 0) > cutoff_time),\n            maxlen=1000\n        )\n    \n    def get_endpoint_metrics(self, endpoint: str) -> Dict[str, Any]:\n        \"\"\"Get detailed metrics for specific endpoint\"\"\"\n        if endpoint not in self.endpoint_stats:\n            return {'error': 'Endpoint not found'}\n        \n        stats = self.endpoint_stats[endpoint]\n        response_times = list(stats['response_times'])\n        \n        if not response_times:\n            return stats\n        \n        return {\n            **stats,\n            'average_response_time': stats['total_time'] / stats['count'],\n            'error_rate': stats['error_count'] / stats['count'],\n            'slow_request_rate': stats['slow_requests'] / stats['count'],\n            'percentiles': {\n                'p50': np.percentile(response_times, 50),\n                'p75': np.percentile(response_times, 75),\n                'p90': np.percentile(response_times, 90),\n                'p95': np.percentile(response_times, 95),\n                'p99': np.percentile(response_times, 99)\n            },\n            'min_response_time': min(response_times),\n            'max_response_time': max(response_times)\n        }\n    \n    def get_alerts(self, severity: Optional[str] = None, limit: int = 50) -> List[Dict]:\n        \"\"\"Get recent alerts with optional filtering\"\"\"\n        alerts = list(self._alerts_queue)\n        \n        if severity:\n            alerts = [alert for alert in alerts if alert.get('severity') == severity]\n        \n        return sorted(alerts, key=lambda x: x['timestamp'], reverse=True)[:limit]


def init_apm_service(app: Flask, redis_client: Optional[redis.Redis] = None) -> APMService:\n    \"\"\"Initialize APM service with Flask app\"\"\"\n    apm_service = APMService(app, redis_client)\n    \n    # Add CLI commands\n    @app.cli.command()\n    def apm_summary():\n        \"\"\"Show APM performance summary\"\"\"\n        summary = apm_service.get_performance_summary()\n        print(json.dumps(summary, indent=2))\n    \n    @app.cli.command()\n    def apm_alerts():\n        \"\"\"Show recent APM alerts\"\"\"\n        alerts = apm_service.get_alerts()\n        for alert in alerts[-10:]:\n            print(f\"{alert['type']}: {alert['data']} (severity: {alert['severity']})\")\n    \n    return apm_service